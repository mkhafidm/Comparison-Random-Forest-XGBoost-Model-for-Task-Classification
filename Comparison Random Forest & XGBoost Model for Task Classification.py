# -*- coding: utf-8 -*-
"""Machine Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EhNZNc1ZWVboKlopDVHOqgha2NeLnCQZ

# **Comparison Random Forest & XGBoost Model for Task Classification**
"""

!pip install rfpimp

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.inspection import permutation_importance
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix

"""## **Data Preparation**"""

url = 'https://drive.google.com/uc?id=1eRWXi6IQRTDoKUgT9-Y_RlFG9cbNN6Oq'
df = pd.read_csv(url)
df

df.info()

df.isnull().sum()

df.duplicated().sum()

"""## **EDA**

### IPM Class Distribution
"""

print('\nDistribution of IPM Class :')
print(df['IPM Class'].value_counts())

plt.figure(figsize=(10, 6))
df['IPM Class'].value_counts().plot(kind='bar')
plt.title('IPM Classification')
plt.xlabel('IPM Class')
plt.ylabel('Count')
plt.show()

"""### IPM Class Distribution in Pie Chart"""

y_axis = df['IPM Class'].value_counts()
my_labels = ['Tinggi','Sangat Tinggi','Sedang']
color = ['#B5C0D0', '#F5E8DD', '#CCD3CA', '#EED3D9']
explode = [0, 0, 0.2]

plt.pie(y_axis, labels=my_labels, explode=explode,
        colors=color, autopct='%1.1f%%')
plt.show()

print('\nDistribution of IPM Classification:')
print(df['IPM Class'].value_counts())

"""### Visualize Kabupaten/Kota "Sedang"
"""

kota_sedang = df.loc[df["IPM Class"] == "Sedang"].sort_values(by='IPM', ascending=True)
y_a = kota_sedang["IPM"]
x_a = kota_sedang["Kabupaten/Kota"]
bar_colors = '#55AD9B'

plt.figure(figsize=(12, 6))
plt.bar(x_a, y_a, color=bar_colors)
plt.title('Kabupaten/Kota yang memiliki nilai IPM sedang')
plt.xlabel('Kabupaten/Kota')
plt.ylabel('IPM')
plt.xticks(rotation=45)
plt.show()

"""### IPM Distribution"""

plt.figure(figsize=(10, 6))
sns.histplot(df['IPM'], color='#FFDE95', bins=20)
plt.title('Distribution of IPM')
plt.xlabel('IPM')
plt.ylabel('Count')
plt.show()

"""### Visualize Features Correlation with Heatmap"""

sns.heatmap(df[["Umur Harapan Hidup", "Harapan Lama Sekolah",
                "Rata-Rata Lama Sekolah", "Pengeluaran Per Kapita", "IPM"]].corr(), annot=True, cmap="viridis")

# How many of these values are there

df['IPM Class'].value_counts()

"""Merubah string di IPM Class menjadi integer sesuai dengan `map_legend`."""

# 0 = Sangat Tinggi / 1 = Tinggi / 2 = Sedang
map_legend = {
    'Sangat Tinggi' : 0,
    'Tinggi' : 1,
    'Sedang' : 2
}

df["IPM Class"] = df["IPM Class"].map(map_legend)

df['IPM Class'].value_counts()

# Daftar kolom yang akan dihapus
columns_to_drop = ['IPM Class', 'Kabupaten/Kota', 'IPM']

# Menghapus kolom yang ditentukan
X = df.drop(columns=columns_to_drop)

X.head()

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN
from xgboost import XGBClassifier

y = df["IPM Class"]

# I divide the dataset into training and test data
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 1)


# Scaling the data.
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# let's see the latest data quantities
print("X_train_scaled shape:",X_train.shape)
print("X_test_scaled shape:",X_test.shape)
print("y_train shape:",y_train.shape)
print("y_test shape:",y_test.shape)

"""## Training Model

Data dibagi menjadi 5 bagian dengan Stratified K-fold

https://towardsdatascience.com/what-is-stratified-cross-validation-in-machine-learning-8844f3e7ae8e
"""

skf = StratifiedKFold(n_splits=5)

kfold_split = list(skf.split(X, y))

print(f'PROPORTION OF TARGET IN THE ORIGINAL DATA\n{df["IPM Class"].value_counts() / len(df)}\n\n')
for n,(train_index,test_index) in enumerate(kfold_split):
    print(f'SPLIT NO {n+1}\nTRAINING SET SIZE: {np.round(len(train_index) / (len(train_index)+len(test_index)),2)}'+
          f'\tTEST SET SIZE: {np.round(len(test_index) / (len(train_index)+len(test_index)),2)}\nPROPORTION OF TARGET IN THE TRAINING SET\n'+
          f'{df.iloc[test_index,6].value_counts() / len(df.iloc[test_index,6])}\nPROPORTION OF TARGET IN THE TEST SET\n'+
          f'{df.iloc[train_index,6].value_counts() / len(df.iloc[train_index,6])}\n\n')

"""### Random Forest

Feature importance untuk random forest berdasarkan https://explained.ai/rf-importance/index.html.
"""

from rfpimp import *

skf_score = []
skf_imps = []
for i, (train_index, test_index) in enumerate(kfold_split):
    print(f"SPLIT ke - {i+1}")
    X_train, y_train = X.loc[train_index], y.loc[train_index]
    X_test, y_test = X.loc[test_index], y.loc[test_index]

    clf = RandomForestClassifier(max_depth=5, random_state=0)
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"Akurasi   : {accuracy * 100:.4f}%")

    precision = precision_score(y_test, y_pred, average='macro')
    print(f"Precision : {precision * 100:.4f}%")

    recall = recall_score(y_test, y_pred, average='macro')
    print(f"Recall    : {recall * 100:.4f}%")

    f1_value = f1_score(y_test, y_pred, average="macro")
    print(f"F1 score  : {f1_value * 100:.4f}%")
    skf_score.append([i+1, accuracy, precision, recall, f1_value])
    print()

    # Plot confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap="YlGnBu_r")
    plt.title(f"Confusion Matrix - Split {i+1}")
    plt.show()

    # Plot importance score
    print()
    imp = importances(model=clf, X_valid=X_test, y_valid=y_test)
    skf_imps.append(imp.T)
    viz = plot_importances(imp)
    viz.view()

skf_rf = pd.DataFrame(skf_score, columns=["Iteration", "Accuracy", "Precision", "Recall", "F1 score"])
skf_rf.describe()

pd.concat(skf_imps).describe()

"""### Random Forest + Oversampling (metode ADASYN)"""

from rfpimp import *

skf_score = []
skf_imps = []
for i, (train_index, test_index) in enumerate(kfold_split):
    print(f"SPLIT ke - {i+1}")
    ros = ADASYN(random_state=0)

    X_train, y_train = X.loc[train_index], y.loc[train_index]
    X_test, y_test = X.loc[test_index], y.loc[test_index]

    X_train, y_train = ros.fit_resample(X_train, y_train)

    clf = RandomForestClassifier(max_depth=5, random_state=0)
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"Akurasi   : {accuracy * 100:.4f}%")

    precision = precision_score(y_test, y_pred, average='macro')
    print(f"Precision : {precision * 100:.4f}%")

    recall = recall_score(y_test, y_pred, average='macro')
    print(f"Recall    : {recall * 100:.4f}%")

    f1_value = f1_score(y_test, y_pred, average="macro")
    print(f"F1 score  : {f1_value * 100:.4f}%")
    skf_score.append([i+1, accuracy, precision, recall, f1_value])
    print()

    # Plot confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap="YlGnBu_r")
    plt.title(f"Confusion Matrix - Split {i+1}")
    plt.show()

    # Plot importance score
    print()
    imp = importances(model=clf, X_valid=X_test, y_valid=y_test)
    skf_imps.append(imp.T)
    viz = plot_importances(imp)
    viz.view()

skf_ov = pd.DataFrame(skf_score, columns=["Iteration", "Accuracy", "Precision", "Recall", "F1 score"])
skf_ov.describe()

pd.concat(skf_imps).mean().plot.bar()

"""### XGboost Classification"""

X.columns

skf_score = []
skf_imps = []

for i, (train_index, test_index) in enumerate(kfold_split):
    print(f"SPLIT ke - {i+1}")
    X_train, y_train = X.loc[train_index], y.loc[train_index]
    X_test, y_test = X.loc[test_index], y.loc[test_index]

    xgb = XGBClassifier(max_depth=5, random_state=0)
    xgb.fit(X_train, y_train)

    y_pred = clf.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"Akurasi   : {accuracy * 100:.4f}%")

    precision = precision_score(y_test, y_pred, average='macro')
    print(f"Precision : {precision * 100:.4f}%")

    recall = recall_score(y_test, y_pred, average='macro')
    print(f"Recall    : {recall * 100:.4f}%")

    f1_value = f1_score(y_test, y_pred, average="macro")
    print(f"F1 score  : {f1_value * 100:.4f}%")
    skf_score.append([i+1, accuracy, precision, recall, f1_value])
    print()

    # Plot confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap="YlGnBu_r")
    plt.title(f"Confusion Matrix - Split {i+1}")
    plt.show()

    # Feature importance
    perm_importance = permutation_importance(xgb, X_test, y_test)
    skf_imps.append(pd.DataFrame([perm_importance.importances_mean], columns=X.columns))

skf_xgb = pd.DataFrame(skf_score, columns=["Iteration", "Accuracy", "Precision", "Recall", "F1 score"])
skf_xgb.describe()

pd.concat(skf_imps).describe()

"""### XGboost Classification + Oversampling (ADASYN)"""

skf_score = []
skf_imps = []

for i, (train_index, test_index) in enumerate(kfold_split):
    print(f"SPLIT ke - {i+1}")
    ros = ADASYN(random_state=0)

    X_train, y_train = X.loc[train_index], y.loc[train_index]
    X_test, y_test = X.loc[test_index], y.loc[test_index]

    X_train, y_train = ros.fit_resample(X_train, y_train)

    xgb = XGBClassifier(max_depth=5, random_state=0)
    xgb.fit(X_train, y_train)

    y_pred = clf.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"Akurasi   : {accuracy * 100:.4f}%")

    precision = precision_score(y_test, y_pred, average='macro')
    print(f"Precision : {precision * 100:.4f}%")

    recall = recall_score(y_test, y_pred, average='macro')
    print(f"Recall    : {recall * 100:.4f}%")

    f1_value = f1_score(y_test, y_pred, average="macro")
    print(f"F1 score  : {f1_value * 100:.4f}%")
    skf_score.append([i+1, accuracy, precision, recall, f1_value])
    print()

    # Plot confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap="YlGnBu_r")
    plt.title(f"Confusion Matrix - Split {i+1}")
    plt.show()

    # Feature importance
    perm_importance = permutation_importance(xgb, X_test, y_test)
    skf_imps.append(pd.DataFrame([perm_importance.importances_mean], columns=X.columns))

skf_xgb_ov = pd.DataFrame(skf_score, columns=["Iteration", "Accuracy", "Precision", "Recall", "F1 score"])
skf_xgb_ov.describe()

pd.concat(skf_imps).mean().plot.bar()

"""### Comparison model"""

results=pd.DataFrame({'Model':['Random Forest','Random Forest + Oversampling','XGBoost', 'XGboost + Oversampling'],
                    'Accuracy Score':[skf_rf["Accuracy"].mean(),skf_ov["Accuracy"].mean(),skf_xgb["Accuracy"].mean(), skf_xgb_ov["Accuracy"].mean()]})
comparison_df=results
comparison_df=comparison_df.set_index('Model')
comparison_df

# @title Accuracy Score

from matplotlib import pyplot as plt
comparison_df['Accuracy Score'].plot(kind='line', figsize=(8, 4), title='Accuracy Score')
plt.gca().spines[['top', 'right']].set_visible(False)